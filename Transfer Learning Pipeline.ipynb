{"cells":[{"cell_type":"markdown","source":["**Sentiment Analysis on Stanford's 1.6 million tweets - Transfer Learning Pipeline**\n\nThis notebook needs to be run only once. It builds a deep learning model on Stanford's 1.6 million tweets and saves the featurizer ( tokenizer), model architecture and model weights into DBFS to be used on any other unlabeled dataset as part of the transfer learning pipeline."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Now, let us import Stanford's 1.6 million tweets"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/training_1600000_processed_noemoticon-efba6.csv\"\nfile_type = \"csv\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type).load(file_location)\n\ndf = df.toPandas()\ndf.columns = ['sentiment','id','date','flag','user','text']\n\ndf.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df['sentiment'].value_counts()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: 4    800000\n0    800000\nName: sentiment, dtype: int64</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["**Now, let us create a cleaning function. We will remove all the non-alphanumeric texts like punctuations etc. We will also remove the tags using beautiful soup and expand the shortened words like isn't into is not.**"],"metadata":{}},{"cell_type":"markdown","source":["**Now, let us create a cleaning function. We will remove all the non-alphanumeric texts like punctuations etc. We will also remove the tags using beautiful soup and expand the shortened words like isn't into is not.**"],"metadata":{}},{"cell_type":"code","source":["import re\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import WordPunctTokenizer\ntok = WordPunctTokenizer()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["pip install python-Levenshtein"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">%pip install python-Levenshtein is not supported in Databricks notebooks.\n To modify the environment on driver only, you can use %sh. To modify the environment on driver and executors use library utilities. Run dbutils.library.help() for documentation on Databricks library utilities.\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["def textCleaning(text):\n    \"\"\" \n         Summary: \n         Clean text data by removing http/www links, alphanumerics etc, making text in lowercase and removing leading and tailing white spaces.\n       \n         Parameters: \n         text (is passed in a loop), this function requires a series of input, hence this requires a loop (created in the next cell).\n         \n         Returns:\n         Cleaned text\n    \"\"\"\n    \n    # to filter http/www links, alphanumerics etc\n    filters = r'@[A-Za-z0-9_]+|https?://[^ ]+|www.[^ ]+'\n\n    # Creating a dictionary for unshortten words\n    my_dictionary = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\", \"rt\":''}\n\n    myDictionary_pattern = re.compile(r'\\b(' + '|'.join(my_dictionary.keys()) + r')\\b')\n  \n    text = str(text)\n    \n    #Beautiful Soup is a Python package for parsing HTML and XML documents.\n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    try:\n        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        bom_removed = souped\n        \n    #Filtering with our customized filter that we created\n    stripped = re.sub(filters, '', bom_removed)\n    lower_case = stripped.lower()\n    dictionary_handled = myDictionary_pattern.sub(lambda x: my_dictionary[x.group()], lower_case)\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", dictionary_handled)\n    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n    \n    return (\" \".join(words)).strip()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["df.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Now, let us run the cleaning function defined above to clean all our tweets"],"metadata":{}},{"cell_type":"code","source":["#nums = length of the dataframe\nnums = df.shape[0]\nprint (\"Cleaning and parsing the tweets...\\n\")\n\n#creating empty list\nclean_tweet_texts = []\n\n#Passing texts of our dataframe in loop\nfor i in range(nums):\n    if((i+1)%100000 == 0 ):\n        print (\"Tweets %d of %d has been processed\" % ( i+1, nums ))   \n        \n        #collecting cleaned text in a list\n    clean_tweet_texts.append(textCleaning(df['text'].iloc[i]))\nprint('Pro-processing done 👍🏼')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Cleaning and parsing the tweets...\n\nTweets 100000 of 1600000 has been processed\nTweets 200000 of 1600000 has been processed\nTweets 300000 of 1600000 has been processed\nTweets 400000 of 1600000 has been processed\nTweets 500000 of 1600000 has been processed\nTweets 600000 of 1600000 has been processed\nTweets 700000 of 1600000 has been processed\nTweets 800000 of 1600000 has been processed\nTweets 900000 of 1600000 has been processed\nTweets 1000000 of 1600000 has been processed\nTweets 1100000 of 1600000 has been processed\nTweets 1200000 of 1600000 has been processed\nTweets 1300000 of 1600000 has been processed\nTweets 1400000 of 1600000 has been processed\nTweets 1500000 of 1600000 has been processed\nTweets 1600000 of 1600000 has been processed\nPro-processing done 👍🏼\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["\ndf['text'] = pd.Series(clean_tweet_texts)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["df['text'] = df['text'].astype('str')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["tweets = df.copy()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["tweets['sentiment'].value_counts()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: 4    800000\n0    800000\nName: sentiment, dtype: int64</div>"]}}],"execution_count":17},{"cell_type":"code","source":["tweets.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>awww that bummer you shoulda got david carr of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can not update his facebook b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>dived many times for the ball managed to save ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>no it not behaving at all mad why am here beca...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["tweets['sentiment'] = tweets['sentiment'].astype('int')\ntweets['sentiment']=tweets['sentiment'].replace(4,1)\ntweets['sentiment'].value_counts()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: 1    800000\n0    800000\nName: sentiment, dtype: int64</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Stanford's data is balanced, we have 800,000 tweets whose sentiment is 0 and 800,000 tweets whose sentiment is 1\n\nLet us grab only the text and the sentiment column to do our sentiment model building"],"metadata":{}},{"cell_type":"code","source":["tweets = tweets[['text', 'sentiment']]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["tweets.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>awww that bummer you shoulda got david carr of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is upset that he can not update his facebook b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dived many times for the ball managed to save ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>no it not behaving at all mad why am here beca...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["Now, let us start our modelling process. Let us import all the necessary libraries"],"metadata":{}},{"cell_type":"code","source":["##helper libraries\n\nfrom subprocess import check_output\nfrom collections import Counter\nimport gc\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation,Input, Flatten\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D, AveragePooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom keras import metrics\nimport h5py\nfrom keras.models import model_from_json\nimport pickle\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, Model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["tweets.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>awww that bummer you shoulda got david carr of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is upset that he can not update his facebook b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dived many times for the ball managed to save ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>no it not behaving at all mad why am here beca...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["texts = tweets['text']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Let us use keras tokenizer to tokenize the tweets using top 10000 occuring words and let us fit on our tweets. We will also save this tokenizer to be used for feature building on any new dataset."],"metadata":{}},{"cell_type":"code","source":["num_max = 10000\ntok = Tokenizer(num_words=num_max)\ntok.fit_on_texts(texts)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["import sys\nimport os\nimport csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["Since, most of the tweets are betwet 20-50 words length, we will only use max sequence length of 40 for our deep learning model. We will also chose 100 as the dimension of our embedding vector which means each word will have 100 dimentional features whose weights will be learnt with deep learning. lets use a validation split of 0.25 to check accuracy o new data."],"metadata":{}},{"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 40\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.25"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["**Now, let us save the tokenizer using pickle**"],"metadata":{}},{"cell_type":"code","source":["# saving\nwith open('/dbfs/FileStore/tables/pipeline_featurizer.pickle', 'wb') as handle:\n    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["**Now, lets convert our text into numbers using the above tokenizer. We will also use padding to equalize the size of each tweet which is reqquired in deep learning models.**"],"metadata":{}},{"cell_type":"code","source":["sequences = tok.texts_to_sequences(texts)\nword_index = tok.word_index\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["labels = tweets['sentiment']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["labels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', len(texts))\nprint('Shape of label tensor:', len(labels))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Shape of data tensor: 1600000\nShape of label tensor: 1600000\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["Now x_train contains 75% of the dataset.Now, let us define an embedding layer with the size 100 defined above in the variable embedding_dim"],"metadata":{}},{"cell_type":"code","source":["embedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            mask_zero=False,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["print('Traing and validation set number of positive and negative reviews')\nprint (y_train.sum(axis=0))\nprint (y_val.sum(axis=0))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Traing and validation set number of positive and negative reviews\n[600310. 599690.]\n[199690. 200310.]\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Rational of model used.\n\n**LSTM PROS**\n\nRecognize pattern across time and can keep overall information of any text\n\nRNN’s are better when classification is determined by a long-range semantic dependency rather than some local key-phrases  (therefore checking both is necessary, CNN works better for classification in most cases though)\n\n\n**LSTM CONS**\n\nWeightage to earlier words decreases with time even with LSTM\n\nParallel computing not possible ( very costly )\n\nLSTMs are very slow, CNNs are 5-6 times faster than LSTM\n\n**LSTM NLP Applications:**\n\nQuestions-Answering\n\nTranslation ( Any sequence to sequence model)\n\nLanguage modelling\n\n\n\n**CNN PROS**\n\nDetect patterns of multiples sizes (2, 3, or 5 adjacent words )\n\nParallel computation\n\nWeights share in different patterns\n\nWeightage remains same for different patterns ( no time component, therefore earlier patters are remembered)\n\nEg: Patterns could be expressions (word ngrams?) like “I hate”, “very good”\n\n**CNN CONS**\n\nConvolutions lose information about the local order of words; therefore sequence information is lost. ( How to rectify? Later ;) )\n\nText with sentiment or class updates with time. \n\n**Applications:**\n\nClassification task  Importance (pattern) > Importance (pattern)\n\nSentiment Analysis\nSpam Detection\nTopic Categorization\n\n\n**Exceptions:**\n\n**LSTM>CNN**\nEg: I thought it would be a great movie, but it turns out that it was shitty.\n\n**CNN >>> LSTM**\nI was very disappointed with this restaurant. The service was incredibly slow, and the food was mediocre. I will not be back\n\n\n**Rational behind Parameters Tuning**\n\nIs word embeddings required? It turns out it isn’t. Why?\n\nEven without pre-trained embeddings the model performs pretty good and sometimes better.\nLinguistics used in #drones tweets appear multiple times in multiple contexts in 1.6 million data\nTherefore, no need of adding complexity through embeddings\n\nDropout value?\n0.5 performs a bit better for new data, better for generalization\n\nMax/Average pooling ?\nAverage pooling works better because it averages the sentiments of 2 patterns. \nMax will give better results maybe for extreme sentiment/polarity classification ( Heuristic checks required though)\n\nAdam Optimizer instead of stochastic gradient descent? Better performance and faster.\n\nSize of filters: depends on what kind of pattern we want to detect ( size = 2, eg: not good,  3, 4 works perfectly fine too )\n\n\n**Rationale for parameters:**\n\n**MAX_SEQUENCE_LENGTH** :- max sequence length should be chosen according to the average length of the text pr document of our input. Here, we had twitter data so we chose around 40.\n\n**embedding layer size**: embedding layer size depends on the amount of training data: If the training dta is huge, we can increase the size of the embedding layer for more and more new features to be learnt through more data\n\nCNN layer number of filters and filter size parameter: **Number of filters** again depends on the number of context or features you want to learn from a single training daata. This has similar intuition as the embedding layer size\n\n**Filter size** is dependent on the number of continuous words in 1 training data that can define a context or feature.: Here we took 3 because normally 3 words next to each other can define a sentimental context\n\n\n\n**Note**\n\nSuppose we have any other training dataset to build this pipeline like for example **amazon reviews**' dataset, in that case we have see the average number of words in each review and increase the size of the LSTM parameters ( max_sequence length from 40 to maybe 80 depends on the avg number of words). We can also increase the number of filters we are using in our CNN layer since more number of words leads to more number of features."],"metadata":{}},{"cell_type":"code","source":["\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["**Based on the above defined rationale, let us build a deep learning model with a hybri of lstm and cnn to perform sentiment prediction**"],"metadata":{}},{"cell_type":"code","source":["\nx = Bidirectional(LSTM(40, return_sequences=True))(embedded_sequences)\n\nx = Conv1D(64, 3, activation='relu')(x)\nx = AveragePooling1D(pool_size=2)(x)\n\nx = Flatten()(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.5)(x)\npreds = Dense(2, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\nmodel.summary()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 40)                0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 40, 100)           26494100  \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 40, 80)            45120     \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 38, 64)            15424     \n_________________________________________________________________\naverage_pooling1d_1 (Average (None, 19, 64)            0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1216)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                77888     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 26,632,662\nTrainable params: 26,632,662\nNon-trainable params: 0\n_________________________________________________________________\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["Now, let us fit the above defined model for 3 epochs"],"metadata":{}},{"cell_type":"code","source":["model.fit(x_train, y_train, validation_data=(x_val, y_val),\n          epochs=3, batch_size=128) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 1200000 samples, validate on 400000 samples\nEpoch 1/3\n\n    128/1200000 [..............................] - ETA: 14:44:31 - loss: 0.6944 - acc: 0.4844\n    256/1200000 [..............................] - ETA: 7:55:11 - loss: 0.6966 - acc: 0.4531 \n    384/1200000 [..............................] - ETA: 5:38:54 - loss: 0.6951 - acc: 0.4766\n    512/1200000 [..............................] - ETA: 4:31:17 - loss: 0.6947 - acc: 0.4844\n    640/1200000 [..............................] - ETA: 3:49:28 - loss: 0.6942 - acc: 0.4875\n    768/1200000 [..............................] - ETA: 3:21:20 - loss: 0.6939 - acc: 0.4974\n    896/1200000 [..............................] - ETA: 3:02:07 - loss: 0.6938 - acc: 0.5000\n   1024/1200000 [..............................] - ETA: 2:48:05 - loss: 0.6931 - acc: 0.5156\n   1152/1200000 [..............................] - ETA: 2:37:21 - loss: 0.6926 - acc: 0.5191\n   1280/1200000 [..............................] - ETA: 2:29:29 - loss: 0.6922 - acc: 0.5172\n   1408/1200000 [..............................] - ETA: 2:22:00 - loss: 0.6921 - acc: 0.5206\n   1536/1200000 [..............................] - ETA: 2:16:14 - loss: 0.6922 - acc: 0.5202\n   1664/1200000 [..............................] - ETA: 2:12:27 - loss: 0.6918 - acc: 0.5222\n   1792/1200000 [..............................] - ETA: 2:07:54 - loss: 0.6913 - acc: 0.5290\n   1920/1200000 [..............................] - ETA: 2:04:08 - loss: 0.6908 - acc: 0.5318\n   2048/1200000 [..............................] - ETA: 2:00:21 - loss: 0.6907 - acc: 0.5327\n   2176/1200000 [..............................] - ETA: 1:57:00 - loss: 0.6910 - acc: 0.5322\n   2304/1200000 [..............................] - ETA: 1:55:46 - loss: 0.6909 - acc: 0.5334\n   2432/1200000 [..............................] - ETA: 1:53:47 - loss: 0.6909 - acc: 0.5329\n   2560/1200000 [..............................] - ETA: 1:51:40 - loss: 0.6908 - acc: 0.5340\n   2688/1200000 [..............................] - ETA: 1:50:04 - loss: 0.6917 - acc: 0.5298\n   2816/1200000 [..............................] - ETA: 1:47:53 - loss: 0.6920 - acc: 0.5273\n   2944/1200000 [..............................] - ETA: 1:45:50 - loss: 0.6919 - acc: 0.5282\n   3072/1200000 [..............................] - ETA: 1:43:58 - loss: 0.6917 - acc: 0.5299\n   3200/1200000 [..............................] - ETA: 1:42:17 - loss: 0.6917 - acc: 0.5303\n   3328/1200000 [..............................] - ETA: 1:40:49 - loss: 0.6915 - acc: 0.5312\n   3456/1200000 [..............................] - ETA: 1:40:01 - loss: 0.6914 - acc: 0.5321\n   3584/1200000 [..............................] - ETA: 1:39:18 - loss: 0.6912 - acc: 0.5343\n   3712/1200000 [..............................] - ETA: 1:38:05 - loss: 0.6910 - acc: 0.5350\n   3840/1200000 [..............................] - ETA: 1:36:58 - loss: 0.6908 - acc: 0.5367\n   3968/1200000 [..............................] - ETA: 1:36:16 - loss: 0.6906 - acc: 0.5381\n   4096/1200000 [..............................] - ETA: 1:35:10 - loss: 0.6906 - acc: 0.5374\n   4224/1200000 [..............................] - ETA: 1:34:31 - loss: 0.6903 - acc: 0.5388\n   4352/1200000 [..............................] - ETA: 1:34:06 - loss: 0.6895 - acc: 0.5420\n   4480/1200000 [..............................] - ETA: 1:33:14 - loss: 0.6892 - acc: 0.5417\n   4608/1200000 [..............................] - ETA: 1:32:22 - loss: 0.6889 - acc: 0.5423\n   4736/1200000 [..............................] - ETA: 1:31:34 - loss: 0.6886 - acc: 0.5422\n   4864/1200000 [..............................] - ETA: 1:31:00 - loss: 0.6881 - acc: 0.5436\n   4992/1200000 [..............................] - ETA: 1:30:31 - loss: 0.6876 - acc: 0.5453\n   5120/1200000 [..............................] - ETA: 1:29:54 - loss: 0.6872 - acc: 0.5473\n   5248/1200000 [..............................] - ETA: 1:29:29 - loss: 0.6863 - acc: 0.5492\n   5376/1200000 [..............................] - ETA: 1:29:34 - loss: 0.6856 - acc: 0.5502\n   5504/1200000 [..............................] - ETA: 1:29:15 - loss: 0.6847 - acc: 0.5514\n   5632/1200000 [..............................] - ETA: 1:28:45 - loss: 0.6836 - acc: 0.5534\n   5760/1200000 [..............................] - ETA: 1:28:12 - loss: 0.6835 - acc: 0.5542\n   5888/1200000 [..............................] - ETA: 1:27:50 - loss: 0.6828 - acc: 0.5560\n   6016/1200000 [..............................] - ETA: 1:27:49 - loss: 0.6819 - acc: 0.5587\n   6144/1200000 [..............................] - ETA: 1:27:47 - loss: 0.6803 - acc: 0.5604\n   6272/1200000 [..............................] - ETA: 1:27:36 - loss: 0.6785 - acc: 0.5643\n   6400/1200000 [..............................] - ETA: 1:27:16 - loss: 0.6767 - acc: 0.5664\n   6528/1200000 [..............................] - ETA: 1:27:27 - loss: 0.6764 - acc: 0.5679\n   6656/1200000 [..............................] - ETA: 1:27:11 - loss: 0.6751 - acc: 0.5702\n   6784/1200000 [..............................] - ETA: 1:26:55 - loss: 0.6724 - acc: 0.5740\n   6912/1200000 [..............................] - ETA: 1:26:29 - loss: 0.6705 - acc: 0.5765\n   7040/1200000 [..............................] - ETA: 1:26:04 - loss: 0.6678 - acc: 0.5801\n   7168/1200000 [..............................] - ETA: 1:25:37 - loss: 0.6664 - acc: 0.5816\n   7296/1200000 [..............................] - ETA: 1:25:15 - loss: 0.6648 - acc: 0.5836\n   7424/1200000 [..............................] - ETA: 1:24:57 - loss: 0.6636 - acc: 0.5859\n   7552/1200000 [..............................] - ETA: 1:24:48 - loss: 0.6621 - acc: 0.5870\n   7680/1200000 [..............................] - ETA: 1:24:25 - loss: 0.6586 - acc: 0.5904\n   7808/1200000 [..............................] - ETA: 1:24:05 - loss: 0.6575 - acc: 0.5918\n   7936/1200000 [..............................] - ETA: 1:23:46 - loss: 0.6560 - acc: 0.5939\n   8064/1200000 [..............................] - ETA: 1:23:20 - loss: 0.6551 - acc: 0.5960\n   8192/1200000 [..............................] - ETA: 1:23:10 - loss: 0.6538 - acc: 0.5979\n   8320/1200000 [..............................] - ETA: 1:23:00 - loss: 0.6518 - acc: 0.6004\n   8448/1200000 [..............................] - ETA: 1:22:56 - loss: 0.6508 - acc: 0.6023\n   8576/1200000 [..............................] - ETA: 1:22:49 - loss: 0.6488 - acc: 0.6039\n   8704/1200000 [..............................] - ETA: 1:22:33 - loss: 0.6473 - acc: 0.6059\n   8832/1200000 [..............................] - ETA: 1:22:13 - loss: 0.6464 - acc: 0.6082\n   8960/1200000 [..............................] - ETA: 1:21:56 - loss: 0.6444 - acc: 0.6108\n   9088/1200000 [..............................] - ETA: 1:21:44 - loss: 0.6432 - acc: 0.6120\n   9216/1200000 [..............................] - ETA: 1:21:33 - loss: 0.6428 - acc: 0.6125\n   9344/1200000 [..............................] - ETA: 1:21:18 - loss: 0.6414 - acc: 0.6140\n   9472/1200000 [..............................] - ETA: 1:21:02 - loss: 0.6407 - acc: 0.6155\n   9600/1200000 [..............................] - ETA: 1:20:50 - loss: 0.6395 - acc: 0.6172\n   9728/1200000 [..............................] - ETA: 1:20:48 - loss: 0.6386 - acc: 0.6185\n   9856/1200000 [..............................] - ETA: 1:20:44 - loss: 0.6380 - acc: 0.6193\n   9984/1200000 [..............................] - ETA: 1:20:30 - loss: 0.6372 - acc: 0.6206\n  10112/1200000 [..............................] - ETA: 1:20:16 - loss: 0.6361 - acc: 0.6220\n  10240/1200000 [..............................] - ETA: 1:20:01 - loss: 0.6354 - acc: 0.6231\n  10368/1200000 [..............................] - ETA: 1:19:54 - loss: 0.6339 - acc: 0.6247\n  10496/1200000 [..............................] - ETA: 1:19:44 - loss: 0.6330 - acc: 0.6255\n  10624/1200000 [..............................] - ETA: 1:19:32 - loss: 0.6320 - acc: 0.6267\n  10752/1200000 [..............................] - ETA: 1:19:19 - loss: 0.6300 - acc: 0.6286\n  10880/1200000 [..............................] - ETA: 1:19:08 - loss: 0.6287 - acc: 0.6304\n  11008/1200000 [..............................] - ETA: 1:19:05 - loss: 0.6280 - acc: 0.6314\n  11136/1200000 [..............................] - ETA: 1:18:55 - loss: 0.6266 - acc: 0.6329\n  11264/1200000 [..............................] - ETA: 1:18:51 - loss: 0.6257 - acc: 0.6342\n  11392/1200000 [..............................] - ETA: 1:18:48 - loss: 0.6242 - acc: 0.6358\n  11520/1200000 [..............................] - ETA: 1:18:42 - loss: 0.6242 - acc: 0.6366\n  11648/1200000 [..............................] - ETA: 1:18:33 - loss: 0.6236 - acc: 0.6377\n  11776/1200000 [..............................] - ETA: 1:18:30 - loss: 0.6224 - acc: 0.6388\n  11904/1200000 [..............................] - ETA: 1:18:30 - loss: 0.6219 - acc: 0.6397\n  12032/1200000 [..............................] - ETA: 1:18:29 - loss: 0.6207 - acc: 0.6409\n  12160/1200000 [..............................] - ETA: 1:18:48 - loss: 0.6201 - acc: 0.6415\n  12288/1200000 [..............................] - ETA: 1:18:38 - loss: 0.6194 - acc: 0.6423\n  12416/1200000 [..............................] - ETA: 1:18:27 - loss: 0.6183 - acc: 0.6434\n  12544/1200000 [..............................] - ETA: 1:18:17 - loss: 0.6181 - acc: 0.6442\n  12672/1200000 [..............................] - ETA: 1:18:07 - loss: 0.6172 - acc: 0.6450\n  12800/1200000 [..............................] - ETA: 1:18:06 - loss: 0.6163 - acc: 0.6461\n  12928/1200000 [..............................] - ETA: 1:17:59 - loss: 0.6154 - acc: 0.6474\n  13056/1200000 [..............................] - ETA: 1:17:57 - loss: 0.6143 - acc: 0.6487\n  13184/1200000 [..............................] - ETA: 1:17:52 - loss: 0.6136 - acc: 0.6497\n  13312/1200000 [..............................] - ETA: 1:18:03 - loss: 0.6130 - acc: 0.6501\n  13440/1200000 [..............................] - ETA: 1:17:54 - loss: 0.6125 - acc: 0.6507\n  13568/1200000 [..............................] - ETA: 1:18:00 - loss: 0.6112 - acc: 0.6518\n  13696/1200000 [..............................] - ETA: 1:17:53 - loss: 0.6101 - acc: 0.6530\n  13824/1200000 [..............................] - ETA: 1:17:54 - loss: 0.6099 - acc: 0.6536\n  13952/1200000 [..............................] - ETA: 1:17:45 - loss: 0.6092 - acc: 0.6542\n  14080/1200000 [..............................] - ETA: 1:17:37 - loss: 0.6089 - acc: 0.6547\n  14208/1200000 [..............................] - ETA: 1:17:29 - loss: 0.6089 - acc: 0.6552\n  14336/1200000 [..............................] - ETA: 1:17:21 - loss: 0.6080 - acc: 0.6560\n  14464/1200000 [..............................] - ETA: 1:17:14 - loss: 0.6072 - acc: 0.6567\n  14592/1200000 [..............................] - ETA: 1:17:13 - loss: 0.6063 - acc: 0.6576\n  14720/1200000 [..............................] - ETA: 1:17:06 - loss: 0.6057 - acc: 0.6580\n  14848/1200000 [..............................] - ETA: 1:17:04 - loss: 0.6054 - acc: 0.6585\n  14976/1200000 [..............................] - ETA: 1:17:00 - loss: 0.6044 - acc: 0.6597\n  15104/1200000 [..............................] - ETA: 1:16:54 - loss: 0.6036 - acc: 0.6606\n  15232/1200000 [..............................] - ETA: 1:16:47 - loss: 0.6031 - acc: 0.6610\n  15360/1200000 [..............................] - ETA: 1:16:39 - loss: 0.6024 - acc: 0.6617\n  15488/1200000 [..............................] - ETA: 1:16:31 - loss: 0.6017 - acc: 0.6626\n  15616/1200000 [..............................] - ETA: 1:16:23 - loss: 0.6009 - acc: 0.6636\n  15744/1200000 [..............................] - ETA: 1:16:15 - loss: 0.6006 - acc: 0.6642\n  15872/1200000 [..............................] - ETA: 1:16:25 - loss: 0.6003 - acc: 0.6649\n  16000/1200000 [..............................] - ETA: 1:16:18 - loss: 0.5999 - acc: 0.6654\n  16128/1200000 [..............................] - ETA: 1:16:14 - loss: 0.5988 - acc: 0.6665\n  16256/1200000 [..............................] - ETA: 1:16:08 - loss: 0.5978 - acc: 0.6676\n  16384/1200000 [..............................] - ETA: 1:16:01 - loss: 0.5971 - acc: 0.6685\n  16512/1200000 [..............................] - ETA: 1:16:01 - loss: 0.5962 - acc: 0.6694\n  16640/1200000 [..............................] - ETA: 1:16:07 - loss: 0.5950 - acc: 0.6703\n  16768/1200000 [..............................] - ETA: 1:16:00 - loss: 0.5944 - acc: 0.6712\n  16896/1200000 [..............................] - ETA: 1:15:54 - loss: 0.5934 - acc: 0.6720\n  17024/1200000 [..............................] - ETA: 1:16:03 - loss: 0.5930 - acc: 0.6723\n  17152/1200000 [..............................] - ETA: 1:16:00 - loss: 0.5923 - acc: 0.6733\n  17280/1200000 [..............................] - ETA: 1:16:01 - loss: 0.5921 - acc: 0.6737\n  17408/1200000 [..............................] - ETA: 1:15:57 - loss: 0.5915 - acc: 0.6743\n  17536/1200000 [..............................] - ETA: 1:15:53 - loss: 0.5913 - acc: 0.6747\n  17664/1200000 [..............................] - ETA: 1:15:49 - loss: 0.5908 - acc: 0.6756\n  17792/1200000 [..............................] - ETA: 1:15:45 - loss: 0.5900 - acc: 0.6762\n  17920/1200000 [..............................] - ETA: 1:15:45 - loss: 0.5893 - acc: 0.6768\n  18048/1200000 [..............................] - ETA: 1:15:40 - loss: 0.5886 - acc: 0.6777\n  18176/1200000 [..............................] - ETA: 1:15:50 - loss: 0.5880 - acc: 0.6784\n  18304/1200000 [..............................] - ETA: 1:15:45 - loss: 0.5878 - acc: 0.6788\n  18432/1200000 [..............................] - ETA: 1:15:39 - loss: 0.5874 - acc: 0.6792\n  18560/1200000 [..............................] - ETA: 1:15:37 - loss: 0.5866 - acc: 0.6802\n  18688/1200000 [..............................] - ETA: 1:15:37 - loss: 0.5863 - acc: 0.6809\n  18816/1200000 [..............................] - ETA: 1:15:35 - loss: 0.5858 - acc: 0.6815\n  18944/1200000 [..............................] - ETA: 1:15:32 - loss: 0.5856 - acc: 0.6817\n  19072/1200000 [..............................] - ETA: 1:15:27 - loss: 0.5852 - acc: 0.6821\n  19200/1200000 [..............................] - ETA: 1:15:22 - loss: 0.5843 - acc: 0.6827\n  19328/1200000 [..............................] - ETA: 1:15:17 - loss: 0.5837 - acc: 0.6834\n  19456/1200000 [..............................] - ETA: 1:15:19 - loss: 0.5828 - acc: 0.6843\n  19584/1200000 [..............................] - ETA: 1:15:15 - loss: 0.5826 - acc: 0.6846\n  19712/1200000 [..............................] - ETA: 1:15:12 - loss: 0.5821 - acc: 0.6851\n  19840/1200000 [..............................] - ETA: 1:15:08 - loss: 0.5816 - acc: 0.6854\n  19968/1200000 [..............................] - ETA: 1:15:09 - loss: 0.5813 - acc: 0.6856\n  20096/1200000 [..............................] - ETA: 1:15:07 - loss: 0.5808 - acc: 0.6862\n  20224/1200000 [..............................] - ETA: 1:15:05 - loss: 0.5802 - acc: 0.6871\n  20352/1200000 [..............................] - ETA: 1:15:01 - loss: 0.5796 - acc: 0.6875\n  20480/1200000 [..............................] - ETA: 1:14:58 - loss: 0.5789 - acc: 0.6883\n  20608/1200000 [..............................] - ETA: 1:15:03 - loss: 0.5789 - acc: 0.6887\n  20736/1200000 [..............................] - ETA: 1:15:01 - loss: 0.5781 - acc: 0.6894\n  20864/1200000 [..............................] - ETA: 1:14:57 - loss: 0.5776 - acc: 0.6898\n  20992/1200000 [..............................] - ETA: 1:14:52 - loss: 0.5770 - acc: 0.6901\n  21120/1200000 [..............................] - ETA: 1:14:50 - loss: 0.5767 - acc: 0.6902\n  21248/1200000 [..............................] - ETA: 1:14:55 - loss: 0.5769 - acc: 0.6901\n  21376/1200000 [..............................] - ETA: 1:14:51 - loss: 0.5762 - acc: 0.6907\n  21504/1200000 [..............................] - ETA: 1:14:46 - loss: 0.5758 - acc: 0.6911\n  21632/1200000 [..............................] - ETA: 1:14:42 - loss: 0.5750 - acc: 0.6915\n  21760/1200000 [..............................] - ETA: 1:14:36 - loss: 0.5741 - acc: 0.6923\n  21888/1200000 [..............................] - ETA: 1:14:36 - loss: 0.5736 - acc: 0.6928\n  22016/1200000 [..............................] - ETA: 1:14:31 - loss: 0.5732 - acc: 0.6934\n  22144/1200000 [..............................] - ETA: 1:14:30 - loss: 0.5732 - acc: 0.6936\n  22272/1200000 [..............................] - ETA: 1:14:26 - loss: 0.5728 - acc: 0.6941\n  22400/1200000 [..............................] - ETA: 1:14:23 - loss: 0.5719 - acc: 0.6946\n  22528/1200000 [..............................] - ETA: 1:14:21 - loss: 0.5712 - acc: 0.6950\n  22656/1200000 [..............................] - ETA: 1:14:18 - loss: 0.5705 - acc: 0.6956\n  22784/1200000 [..............................] - ETA: 1:14:13 - loss: 0.5699 - acc: 0.6962\n  22912/1200000 [..............................] - ETA: 1:14:11 - loss: 0.5697 - acc: 0.6965\n  23040/1200000 [..............................] - ETA: 1:14:06 - loss: 0.5690 - acc: 0.6972\n  23168/1200000 [..............................] - ETA: 1:14:05 - loss: 0.5688 - acc: 0.6974\n  23296/1200000 [..............................] - ETA: 1:14:02 - loss: 0.5689 - acc: 0.6975\n  23424/1200000 [..............................] - ETA: 1:13:59 - loss: 0.5687 - acc: 0.6979\n  23552/1200000 [..............................] - ETA: 1:13:55 - loss: 0.5684 - acc: 0.6982\n  23680/1200000 [..............................] - ETA: 1:13:55 - loss: 0.5686 - acc: 0.6984\n  23808/1200000 [..............................] - ETA: 1:13:52 - loss: 0.5682 - acc: 0.6988\n  23936/1200000 [..............................] - ETA: 1:13:50 - loss: 0.5683 - acc: 0.6989\n  24064/1200000 [..............................] - ETA: 1:13:53 - loss: 0.5678 - acc: 0.6995\n  24192/1200000 [..............................] - ETA: 1:13:48 - loss: 0.5674 - acc: 0.7001\n  24320/1200000 [..............................] - ETA: 1:13:46 - loss: 0.5671 - acc: 0.7005\n  24448/1200000 [..............................] - ETA: 1:13:43 - loss: 0.5670 - acc: 0.7006\n  24576/1200000 [..............................] - ETA: 1:13:39 - loss: 0.5667 - acc: 0.7009\n  24704/1200000 [..............................] - ETA: 1:13:38 - loss: 0.5660 - acc: 0.7015\n  24832/1200000 [..............................] - ETA: 1:13:36 - loss: 0.5656 - acc: 0.7018\n  24960/1200000 [..............................] - ETA: 1:13:42 - loss: 0.5652 - acc: 0.7024\n  25088/1200000 [..............................] - ETA: 1:13:42 - loss: 0.5649 - acc: 0.7028\n  25216/1200000 [..............................] - ETA: 1:13:48 - loss: 0.5646 - acc: 0.7031\n  25344/1200000 [..............................] - ETA: 1:13:44 - loss: 0.5639 - acc: 0.7035\n  25472/1200000 [..............................] - ETA: 1:13:42 - loss: 0.5639 - acc: 0.7035\n  25600/1200000 [..............................] - ETA: 1:13:37 - loss: 0.5639 - acc: 0.7036\n  25728/1200000 [..............................] - ETA: 1:13:36 - loss: 0.5636 - acc: 0.7038\n  25856/1200000 [..............................] - ETA: 1:13:33 - loss: 0.5633 - acc: 0.7041\n  25984/1200000 [..............................] - ETA: 1:13:31 - loss: 0.5632 - acc: 0.7043\n  26112/1200000 [..............................] - ETA: 1:13:39 - loss: 0.5628 - acc: 0.7047\n  26240/1200000 [..............................] - ETA: 1:13:35 - loss: 0.5628 - acc: 0.7048\n  26368/1200000 [..............................] - ETA: 1:13:32 - loss: 0.5623 - acc: 0.7052\n  26496/1200000 [..............................] - ETA: 1:13:33 - loss: 0.5616 - acc: 0.7058\n  26624/1200000 [..............................] - ETA: 1:13:29 - loss: 0.5610 - acc: 0.7063\n  26752/1200000 [..............................] - ETA: 1:13:31 - loss: 0.5607 - acc: 0.7065\n  26880/1200000 [..............................] - ETA: 1:13:29 - loss: 0.5602 - acc: 0.7070\n  27008/1200000 [..............................] - ETA: 1:13:26 - loss: 0.5602 - acc: 0.7073\n  27136/1200000 [..............................] - ETA: 1:13:23 - loss: 0.5598 - acc: 0.7077\n  27264/1200000 [..............................] - ETA: 1:13:21 - loss: 0.5596 - acc: 0.7079\n  27392/1200000 [..............................] - ETA: 1:13:18 - loss: 0.5593 - acc: 0.7079\n  27520/1200000 [..............................] - ETA: 1:13:17 - loss: 0.5593 - acc: 0.7082\n  27648/1200000 [..............................] - ETA: 1:13:15 - loss: 0.5589 - acc: 0.7085\n  27776/1200000 [..............................] - ETA: 1:13:14 - loss: 0.5592 - acc: 0.7085\n  27904/1200000 [..............................] - ETA: 1:13:11 - loss: 0.5586 - acc: 0.7089\n  28032/1200000 [..............................] - ETA: 1:13:07 - loss: 0.5586 - acc: 0.7089\n  28160/1200000 [..............................] - ETA: 1:13:03 - loss: 0.5583 - acc: 0.7091\n  28288/1200000 [..............................] - ETA: 1:13:03 - loss: 0.5578 - acc: 0.7095\n  28416/1200000 [..............................] - ETA: 1:12:59 - loss: 0.5572 - acc: 0.7100\n  28544/1200000 [..............................] - ETA: 1:12:56 - loss: 0.5568 - acc: 0.7103\n  28672/1200000 [..............................] - ETA: 1:12:57 - loss: 0.5566 - acc: 0.7104\n  28800/1200000 [..............................] - ETA: 1:12:55 - loss: 0.5562 - acc: 0.7106\n  28928/1200000 [..............................] - ETA: 1:12:52 - loss: 0.5559 - acc: 0.7109\n  29056/1200000 [..............................] - ETA: 1:12:53 - loss: 0.5556 - acc: 0.7112\n  29184/1200000 [..............................] - ETA: 1:12:49 - loss: 0.5550 - acc: 0.7118\n  29312/1200000 [..............................] - ETA: 1:12:47 - loss: 0.5549 - acc: 0.7119\n  29440/1200000 [..............................] - ETA: 1:12:44 - loss: 0.5544 - acc: 0.7123\n  29568/1200000 [..............................] - ETA: 1:12:42 - loss: 0.5542 - acc: 0.7126\n  29696/1200000 [..............................] - ETA: 1:12:39 - loss: 0.5538 - acc: 0.7129\n  29824/1200000 [..............................] - ETA: 1:12:38 - loss: 0.5535 - acc: 0.7132\n  29952/1200000 [..............................] - ETA: 1:12:38 - loss: 0.5533 - acc: 0.7133\n  30080/1200000 [..............................] - ETA: 1:12:37 - loss: 0.5531 - acc: 0.7135\n  30208/1200000 [..............................] - ETA: 1:12:35 - loss: 0.5526 - acc: 0.7139\n  30336/1200000 [..............................] - ETA: 1:12:32 - loss: 0.5521 - acc: 0.7143\n  30464/1200000 [..............................] - ETA: 1:12:29 - loss: 0.5517 - acc: 0.7146\n  30592/1200000 [..............................] - ETA: 1:12:25 - loss: 0.5517 - acc: 0.7148\n  30720/1200000 [..............................] - ETA: 1:12:23 - loss: 0.5516 - acc: 0.7149\n  30848/1200000 [..............................] - ETA: 1:12:19 - loss: 0.5511 - acc: 0.7152\n  30976/1200000 [..............................] - ETA: 1:12:17 - loss: 0.5510 - acc: 0.7154\n  31104/1200000 [..............................] - ETA: 1:12:16 - loss: 0.5506 - acc: 0.7157\n  31232/1200000 [..............................] - ETA: 1:12:16 - loss: 0.5506 - acc: 0.7160\n  31360/1200000 [..............................] - ETA: 1:12:14 - loss: 0.5505 - acc: 0.7161\n  31488/1200000 [..............................] - ETA: 1:12:11 - loss: 0.5503 - acc: 0.7162\n  31616/1200000 [..............................] - ETA: 1:12:09 - loss: 0.5502 - acc: 0.7165\n  31744/1200000 [..............................] - ETA: 1:12:07 - loss: 0.5497 - acc: 0.7169\n  31872/1200000 [..............................] - ETA: 1:12:06 - loss: 0.5494 - acc: 0.7172\n  32000/1200000 [..............................] - ETA: 1:12:05 - loss: 0.5491 - acc: 0.7175\n  32128/1200000 [..............................] - ETA: 1:12:02 - loss: 0.5486 - acc: 0.7178\n  32256/1200000 [..............................] - ETA: 1:11:59 - loss: 0.5486 - acc: 0.7179\n  32384/1200000 [..............................] - ETA: 1:11:56 - loss: 0.5485 - acc: 0.7180\n  32512/1200000 [..............................] - ETA: 1:11:58 - loss: 0.5481 - acc: 0.7186\n  32640/1200000 [..............................] - ETA: 1:11:55 - loss: 0.5480 - acc: 0.7188\n  32768/1200000 [..............................] - ETA: 1:11:55 - loss: 0.5477 - acc: 0.7191\n  32896/1200000 [..............................] - ETA: 1:11:53 - loss: 0.5473 - acc: 0.7194\n  33024/1200000 [..............................] - ETA: 1:12:01 - loss: 0.5470 - acc: 0.7195\n  33152/1200000 [..............................] - ETA: 1:12:01 - loss: 0.5468 - acc: 0.7197\n  33280/1200000 [..............................] - ETA: 1:11:57 - loss: 0.5465 - acc: 0.7198\n  33408/1200000 [..............................] - ETA: 1:11:55 - loss: 0.5463 - acc: 0.7200\n  33536/1200000 [..............................] - ETA: 1:11:51 - loss: 0.5463 - acc: 0.7201\n  33664/1200000 [..............................] - ETA: 1:11:54 - loss: 0.5461 - acc: 0.7203\n  33792/1200000 [..............................] - ETA: 1:11:52 - loss: 0.5457 - acc: 0.7207\n  33920/1200000 [..............................] - ETA: 1:11:50 - loss: 0.5453 - acc: 0.7209\n  34048/1200000 [..............................] - ETA: 1:11:48 - loss: 0.5452 - acc: 0.7210\n\n*** WARNING: skipped 2515786 bytes of output ***\n\n1164544/1200000 [============================&gt;.] - ETA: 2:09 - loss: 0.3670 - acc: 0.8383\n1164672/1200000 [============================&gt;.] - ETA: 2:09 - loss: 0.3670 - acc: 0.8383\n1164800/1200000 [============================&gt;.] - ETA: 2:09 - loss: 0.3670 - acc: 0.8383\n1164928/1200000 [============================&gt;.] - ETA: 2:08 - loss: 0.3670 - acc: 0.8383\n1165056/1200000 [============================&gt;.] - ETA: 2:08 - loss: 0.3670 - acc: 0.8383\n1165184/1200000 [============================&gt;.] - ETA: 2:07 - loss: 0.3670 - acc: 0.8383\n1165312/1200000 [============================&gt;.] - ETA: 2:07 - loss: 0.3670 - acc: 0.8383\n1165440/1200000 [============================&gt;.] - ETA: 2:06 - loss: 0.3670 - acc: 0.8383\n1165568/1200000 [============================&gt;.] - ETA: 2:06 - loss: 0.3670 - acc: 0.8383\n1165696/1200000 [============================&gt;.] - ETA: 2:05 - loss: 0.3670 - acc: 0.8383\n1165824/1200000 [============================&gt;.] - ETA: 2:05 - loss: 0.3670 - acc: 0.8383\n1165952/1200000 [============================&gt;.] - ETA: 2:04 - loss: 0.3670 - acc: 0.8383\n1166080/1200000 [============================&gt;.] - ETA: 2:04 - loss: 0.3670 - acc: 0.8383\n1166208/1200000 [============================&gt;.] - ETA: 2:03 - loss: 0.3670 - acc: 0.8383\n1166336/1200000 [============================&gt;.] - ETA: 2:03 - loss: 0.3670 - acc: 0.8383\n1166464/1200000 [============================&gt;.] - ETA: 2:02 - loss: 0.3670 - acc: 0.8383\n1166592/1200000 [============================&gt;.] - ETA: 2:02 - loss: 0.3670 - acc: 0.8383\n1166720/1200000 [============================&gt;.] - ETA: 2:02 - loss: 0.3670 - acc: 0.8383\n1166848/1200000 [============================&gt;.] - ETA: 2:01 - loss: 0.3670 - acc: 0.8383\n1166976/1200000 [============================&gt;.] - ETA: 2:01 - loss: 0.3670 - acc: 0.8383\n1167104/1200000 [============================&gt;.] - ETA: 2:00 - loss: 0.3670 - acc: 0.8383\n1167232/1200000 [============================&gt;.] - ETA: 2:00 - loss: 0.3670 - acc: 0.8383\n1167360/1200000 [============================&gt;.] - ETA: 1:59 - loss: 0.3670 - acc: 0.8383\n1167488/1200000 [============================&gt;.] - ETA: 1:59 - loss: 0.3670 - acc: 0.8383\n1167616/1200000 [============================&gt;.] - ETA: 1:58 - loss: 0.3670 - acc: 0.8383\n1167744/1200000 [============================&gt;.] - ETA: 1:58 - loss: 0.3670 - acc: 0.8383\n1167872/1200000 [============================&gt;.] - ETA: 1:57 - loss: 0.3670 - acc: 0.8383\n1168000/1200000 [============================&gt;.] - ETA: 1:57 - loss: 0.3670 - acc: 0.8383\n1168128/1200000 [============================&gt;.] - ETA: 1:56 - loss: 0.3670 - acc: 0.8383\n1168256/1200000 [============================&gt;.] - ETA: 1:56 - loss: 0.3671 - acc: 0.8383\n1168384/1200000 [============================&gt;.] - ETA: 1:55 - loss: 0.3671 - acc: 0.8383\n1168512/1200000 [============================&gt;.] - ETA: 1:55 - loss: 0.3670 - acc: 0.8383\n1168640/1200000 [============================&gt;.] - ETA: 1:54 - loss: 0.3670 - acc: 0.8383\n1168768/1200000 [============================&gt;.] - ETA: 1:54 - loss: 0.3670 - acc: 0.8383\n1168896/1200000 [============================&gt;.] - ETA: 1:54 - loss: 0.3670 - acc: 0.8383\n1169024/1200000 [============================&gt;.] - ETA: 1:53 - loss: 0.3671 - acc: 0.8383\n1169152/1200000 [============================&gt;.] - ETA: 1:53 - loss: 0.3671 - acc: 0.8383\n1169280/1200000 [============================&gt;.] - ETA: 1:52 - loss: 0.3671 - acc: 0.8383\n1169408/1200000 [============================&gt;.] - ETA: 1:52 - loss: 0.3671 - acc: 0.8383\n1169536/1200000 [============================&gt;.] - ETA: 1:51 - loss: 0.3671 - acc: 0.8383\n1169664/1200000 [============================&gt;.] - ETA: 1:51 - loss: 0.3671 - acc: 0.8383\n1169792/1200000 [============================&gt;.] - ETA: 1:50 - loss: 0.3671 - acc: 0.8383\n1169920/1200000 [============================&gt;.] - ETA: 1:50 - loss: 0.3671 - acc: 0.8383\n1170048/1200000 [============================&gt;.] - ETA: 1:49 - loss: 0.3671 - acc: 0.8383\n1170176/1200000 [============================&gt;.] - ETA: 1:49 - loss: 0.3671 - acc: 0.8383\n1170304/1200000 [============================&gt;.] - ETA: 1:48 - loss: 0.3671 - acc: 0.8383\n1170432/1200000 [============================&gt;.] - ETA: 1:48 - loss: 0.3671 - acc: 0.8383\n1170560/1200000 [============================&gt;.] - ETA: 1:47 - loss: 0.3671 - acc: 0.8383\n1170688/1200000 [============================&gt;.] - ETA: 1:47 - loss: 0.3671 - acc: 0.8383\n1170816/1200000 [============================&gt;.] - ETA: 1:47 - loss: 0.3671 - acc: 0.8383\n1170944/1200000 [============================&gt;.] - ETA: 1:46 - loss: 0.3671 - acc: 0.8383\n1171072/1200000 [============================&gt;.] - ETA: 1:46 - loss: 0.3671 - acc: 0.8383\n1171200/1200000 [============================&gt;.] - ETA: 1:45 - loss: 0.3671 - acc: 0.8383\n1171328/1200000 [============================&gt;.] - ETA: 1:45 - loss: 0.3671 - acc: 0.8383\n1171456/1200000 [============================&gt;.] - ETA: 1:44 - loss: 0.3671 - acc: 0.8383\n1171584/1200000 [============================&gt;.] - ETA: 1:44 - loss: 0.3671 - acc: 0.8383\n1171712/1200000 [============================&gt;.] - ETA: 1:43 - loss: 0.3671 - acc: 0.8383\n1171840/1200000 [============================&gt;.] - ETA: 1:43 - loss: 0.3671 - acc: 0.8383\n1171968/1200000 [============================&gt;.] - ETA: 1:42 - loss: 0.3671 - acc: 0.8383\n1172096/1200000 [============================&gt;.] - ETA: 1:42 - loss: 0.3671 - acc: 0.8383\n1172224/1200000 [============================&gt;.] - ETA: 1:41 - loss: 0.3671 - acc: 0.8383\n1172352/1200000 [============================&gt;.] - ETA: 1:41 - loss: 0.3671 - acc: 0.8383\n1172480/1200000 [============================&gt;.] - ETA: 1:40 - loss: 0.3671 - acc: 0.8383\n1172608/1200000 [============================&gt;.] - ETA: 1:40 - loss: 0.3671 - acc: 0.8383\n1172736/1200000 [============================&gt;.] - ETA: 1:39 - loss: 0.3671 - acc: 0.8383\n1172864/1200000 [============================&gt;.] - ETA: 1:39 - loss: 0.3671 - acc: 0.8383\n1172992/1200000 [============================&gt;.] - ETA: 1:39 - loss: 0.3671 - acc: 0.8383\n1173120/1200000 [============================&gt;.] - ETA: 1:38 - loss: 0.3671 - acc: 0.8383\n1173248/1200000 [============================&gt;.] - ETA: 1:38 - loss: 0.3671 - acc: 0.8383\n1173376/1200000 [============================&gt;.] - ETA: 1:37 - loss: 0.3671 - acc: 0.8383\n1173504/1200000 [============================&gt;.] - ETA: 1:37 - loss: 0.3671 - acc: 0.8383\n1173632/1200000 [============================&gt;.] - ETA: 1:36 - loss: 0.3671 - acc: 0.8383\n1173760/1200000 [============================&gt;.] - ETA: 1:36 - loss: 0.3671 - acc: 0.8383\n1173888/1200000 [============================&gt;.] - ETA: 1:35 - loss: 0.3671 - acc: 0.8383\n1174016/1200000 [============================&gt;.] - ETA: 1:35 - loss: 0.3671 - acc: 0.8383\n1174144/1200000 [============================&gt;.] - ETA: 1:34 - loss: 0.3671 - acc: 0.8383\n1174272/1200000 [============================&gt;.] - ETA: 1:34 - loss: 0.3671 - acc: 0.8383\n1174400/1200000 [============================&gt;.] - ETA: 1:33 - loss: 0.3671 - acc: 0.8383\n1174528/1200000 [============================&gt;.] - ETA: 1:33 - loss: 0.3671 - acc: 0.8383\n1174656/1200000 [============================&gt;.] - ETA: 1:32 - loss: 0.3671 - acc: 0.8383\n1174784/1200000 [============================&gt;.] - ETA: 1:32 - loss: 0.3671 - acc: 0.8383\n1174912/1200000 [============================&gt;.] - ETA: 1:32 - loss: 0.3671 - acc: 0.8383\n1175040/1200000 [============================&gt;.] - ETA: 1:31 - loss: 0.3671 - acc: 0.8383\n1175168/1200000 [============================&gt;.] - ETA: 1:31 - loss: 0.3671 - acc: 0.8383\n1175296/1200000 [============================&gt;.] - ETA: 1:30 - loss: 0.3671 - acc: 0.8383\n1175424/1200000 [============================&gt;.] - ETA: 1:30 - loss: 0.3671 - acc: 0.8383\n1175552/1200000 [============================&gt;.] - ETA: 1:29 - loss: 0.3671 - acc: 0.8383\n1175680/1200000 [============================&gt;.] - ETA: 1:29 - loss: 0.3671 - acc: 0.8383\n1175808/1200000 [============================&gt;.] - ETA: 1:28 - loss: 0.3671 - acc: 0.8383\n1175936/1200000 [============================&gt;.] - ETA: 1:28 - loss: 0.3671 - acc: 0.8383\n1176064/1200000 [============================&gt;.] - ETA: 1:27 - loss: 0.3671 - acc: 0.8383\n1176192/1200000 [============================&gt;.] - ETA: 1:27 - loss: 0.3671 - acc: 0.8383\n1176320/1200000 [============================&gt;.] - ETA: 1:26 - loss: 0.3671 - acc: 0.8383\n1176448/1200000 [============================&gt;.] - ETA: 1:26 - loss: 0.3671 - acc: 0.8383\n1176576/1200000 [============================&gt;.] - ETA: 1:25 - loss: 0.3671 - acc: 0.8383\n1176704/1200000 [============================&gt;.] - ETA: 1:25 - loss: 0.3671 - acc: 0.8383\n1176832/1200000 [============================&gt;.] - ETA: 1:24 - loss: 0.3671 - acc: 0.8383\n1176960/1200000 [============================&gt;.] - ETA: 1:24 - loss: 0.3671 - acc: 0.8383\n1177088/1200000 [============================&gt;.] - ETA: 1:24 - loss: 0.3671 - acc: 0.8383\n1177216/1200000 [============================&gt;.] - ETA: 1:23 - loss: 0.3671 - acc: 0.8383\n1177344/1200000 [============================&gt;.] - ETA: 1:23 - loss: 0.3671 - acc: 0.8383\n1177472/1200000 [============================&gt;.] - ETA: 1:22 - loss: 0.3671 - acc: 0.8383\n1177600/1200000 [============================&gt;.] - ETA: 1:22 - loss: 0.3671 - acc: 0.8383\n1177728/1200000 [============================&gt;.] - ETA: 1:21 - loss: 0.3671 - acc: 0.8383\n1177856/1200000 [============================&gt;.] - ETA: 1:21 - loss: 0.3671 - acc: 0.8383\n1177984/1200000 [============================&gt;.] - ETA: 1:20 - loss: 0.3671 - acc: 0.8383\n1178112/1200000 [============================&gt;.] - ETA: 1:20 - loss: 0.3671 - acc: 0.8383\n1178240/1200000 [============================&gt;.] - ETA: 1:19 - loss: 0.3671 - acc: 0.8383\n1178368/1200000 [============================&gt;.] - ETA: 1:19 - loss: 0.3671 - acc: 0.8383\n1178496/1200000 [============================&gt;.] - ETA: 1:18 - loss: 0.3671 - acc: 0.8383\n1178624/1200000 [============================&gt;.] - ETA: 1:18 - loss: 0.3671 - acc: 0.8383\n1178752/1200000 [============================&gt;.] - ETA: 1:17 - loss: 0.3671 - acc: 0.8383\n1178880/1200000 [============================&gt;.] - ETA: 1:17 - loss: 0.3671 - acc: 0.8383\n1179008/1200000 [============================&gt;.] - ETA: 1:16 - loss: 0.3671 - acc: 0.8383\n1179136/1200000 [============================&gt;.] - ETA: 1:16 - loss: 0.3671 - acc: 0.8383\n1179264/1200000 [============================&gt;.] - ETA: 1:16 - loss: 0.3671 - acc: 0.8383\n1179392/1200000 [============================&gt;.] - ETA: 1:15 - loss: 0.3671 - acc: 0.8383\n1179520/1200000 [============================&gt;.] - ETA: 1:15 - loss: 0.3671 - acc: 0.8383\n1179648/1200000 [============================&gt;.] - ETA: 1:14 - loss: 0.3671 - acc: 0.8383\n1179776/1200000 [============================&gt;.] - ETA: 1:14 - loss: 0.3671 - acc: 0.8383\n1179904/1200000 [============================&gt;.] - ETA: 1:13 - loss: 0.3671 - acc: 0.8383\n1180032/1200000 [============================&gt;.] - ETA: 1:13 - loss: 0.3671 - acc: 0.8383\n1180160/1200000 [============================&gt;.] - ETA: 1:12 - loss: 0.3671 - acc: 0.8383\n1180288/1200000 [============================&gt;.] - ETA: 1:12 - loss: 0.3671 - acc: 0.8383\n1180416/1200000 [============================&gt;.] - ETA: 1:11 - loss: 0.3671 - acc: 0.8383\n1180544/1200000 [============================&gt;.] - ETA: 1:11 - loss: 0.3671 - acc: 0.8383\n1180672/1200000 [============================&gt;.] - ETA: 1:10 - loss: 0.3671 - acc: 0.8383\n1180800/1200000 [============================&gt;.] - ETA: 1:10 - loss: 0.3671 - acc: 0.8383\n1180928/1200000 [============================&gt;.] - ETA: 1:09 - loss: 0.3671 - acc: 0.8383\n1181056/1200000 [============================&gt;.] - ETA: 1:09 - loss: 0.3671 - acc: 0.8383\n1181184/1200000 [============================&gt;.] - ETA: 1:08 - loss: 0.3671 - acc: 0.8383\n1181312/1200000 [============================&gt;.] - ETA: 1:08 - loss: 0.3671 - acc: 0.8383\n1181440/1200000 [============================&gt;.] - ETA: 1:08 - loss: 0.3671 - acc: 0.8383\n1181568/1200000 [============================&gt;.] - ETA: 1:07 - loss: 0.3671 - acc: 0.8383\n1181696/1200000 [============================&gt;.] - ETA: 1:07 - loss: 0.3671 - acc: 0.8383\n1181824/1200000 [============================&gt;.] - ETA: 1:06 - loss: 0.3671 - acc: 0.8383\n1181952/1200000 [============================&gt;.] - ETA: 1:06 - loss: 0.3671 - acc: 0.8383\n1182080/1200000 [============================&gt;.] - ETA: 1:05 - loss: 0.3671 - acc: 0.8383\n1182208/1200000 [============================&gt;.] - ETA: 1:05 - loss: 0.3671 - acc: 0.8383\n1182336/1200000 [============================&gt;.] - ETA: 1:04 - loss: 0.3671 - acc: 0.8383\n1182464/1200000 [============================&gt;.] - ETA: 1:04 - loss: 0.3671 - acc: 0.8383\n1182592/1200000 [============================&gt;.] - ETA: 1:03 - loss: 0.3671 - acc: 0.8383\n1182720/1200000 [============================&gt;.] - ETA: 1:03 - loss: 0.3671 - acc: 0.8383\n1182848/1200000 [============================&gt;.] - ETA: 1:02 - loss: 0.3671 - acc: 0.8383\n1182976/1200000 [============================&gt;.] - ETA: 1:02 - loss: 0.3671 - acc: 0.8383\n1183104/1200000 [============================&gt;.] - ETA: 1:01 - loss: 0.3671 - acc: 0.8383\n1183232/1200000 [============================&gt;.] - ETA: 1:01 - loss: 0.3671 - acc: 0.8383\n1183360/1200000 [============================&gt;.] - ETA: 1:01 - loss: 0.3671 - acc: 0.8383\n1183488/1200000 [============================&gt;.] - ETA: 1:00 - loss: 0.3671 - acc: 0.8383\n1183616/1200000 [============================&gt;.] - ETA: 1:00 - loss: 0.3671 - acc: 0.8383\n1183744/1200000 [============================&gt;.] - ETA: 59s - loss: 0.3671 - acc: 0.8383 \n1183872/1200000 [============================&gt;.] - ETA: 59s - loss: 0.3671 - acc: 0.8383\n1184000/1200000 [============================&gt;.] - ETA: 58s - loss: 0.3671 - acc: 0.8383\n1184128/1200000 [============================&gt;.] - ETA: 58s - loss: 0.3671 - acc: 0.8383\n1184256/1200000 [============================&gt;.] - ETA: 57s - loss: 0.3671 - acc: 0.8383\n1184384/1200000 [============================&gt;.] - ETA: 57s - loss: 0.3671 - acc: 0.8383\n1184512/1200000 [============================&gt;.] - ETA: 56s - loss: 0.3671 - acc: 0.8383\n1184640/1200000 [============================&gt;.] - ETA: 56s - loss: 0.3671 - acc: 0.8383\n1184768/1200000 [============================&gt;.] - ETA: 55s - loss: 0.3671 - acc: 0.8383\n1184896/1200000 [============================&gt;.] - ETA: 55s - loss: 0.3671 - acc: 0.8383\n1185024/1200000 [============================&gt;.] - ETA: 54s - loss: 0.3671 - acc: 0.8383\n1185152/1200000 [============================&gt;.] - ETA: 54s - loss: 0.3671 - acc: 0.8383\n1185280/1200000 [============================&gt;.] - ETA: 53s - loss: 0.3671 - acc: 0.8383\n1185408/1200000 [============================&gt;.] - ETA: 53s - loss: 0.3671 - acc: 0.8383\n1185536/1200000 [============================&gt;.] - ETA: 53s - loss: 0.3671 - acc: 0.8383\n1185664/1200000 [============================&gt;.] - ETA: 52s - loss: 0.3671 - acc: 0.8383\n1185792/1200000 [============================&gt;.] - ETA: 52s - loss: 0.3671 - acc: 0.8383\n1185920/1200000 [============================&gt;.] - ETA: 51s - loss: 0.3671 - acc: 0.8383\n1186048/1200000 [============================&gt;.] - ETA: 51s - loss: 0.3671 - acc: 0.8383\n1186176/1200000 [============================&gt;.] - ETA: 50s - loss: 0.3671 - acc: 0.8383\n1186304/1200000 [============================&gt;.] - ETA: 50s - loss: 0.3671 - acc: 0.8383\n1186432/1200000 [============================&gt;.] - ETA: 49s - loss: 0.3671 - acc: 0.8383\n1186560/1200000 [============================&gt;.] - ETA: 49s - loss: 0.3671 - acc: 0.8383\n1186688/1200000 [============================&gt;.] - ETA: 48s - loss: 0.3671 - acc: 0.8383\n1186816/1200000 [============================&gt;.] - ETA: 48s - loss: 0.3671 - acc: 0.8383\n1186944/1200000 [============================&gt;.] - ETA: 47s - loss: 0.3671 - acc: 0.8383\n1187072/1200000 [============================&gt;.] - ETA: 47s - loss: 0.3671 - acc: 0.8383\n1187200/1200000 [============================&gt;.] - ETA: 46s - loss: 0.3671 - acc: 0.8383\n1187328/1200000 [============================&gt;.] - ETA: 46s - loss: 0.3670 - acc: 0.8383\n1187456/1200000 [============================&gt;.] - ETA: 46s - loss: 0.3670 - acc: 0.8383\n1187584/1200000 [============================&gt;.] - ETA: 45s - loss: 0.3671 - acc: 0.8383\n1187712/1200000 [============================&gt;.] - ETA: 45s - loss: 0.3670 - acc: 0.8383\n1187840/1200000 [============================&gt;.] - ETA: 44s - loss: 0.3670 - acc: 0.8383\n1187968/1200000 [============================&gt;.] - ETA: 44s - loss: 0.3670 - acc: 0.8383\n1188096/1200000 [============================&gt;.] - ETA: 43s - loss: 0.3670 - acc: 0.8383\n1188224/1200000 [============================&gt;.] - ETA: 43s - loss: 0.3670 - acc: 0.8383\n1188352/1200000 [============================&gt;.] - ETA: 42s - loss: 0.3671 - acc: 0.8383\n1188480/1200000 [============================&gt;.] - ETA: 42s - loss: 0.3671 - acc: 0.8383\n1188608/1200000 [============================&gt;.] - ETA: 41s - loss: 0.3671 - acc: 0.8383\n1188736/1200000 [============================&gt;.] - ETA: 41s - loss: 0.3671 - acc: 0.8383\n1188864/1200000 [============================&gt;.] - ETA: 40s - loss: 0.3671 - acc: 0.8383\n1188992/1200000 [============================&gt;.] - ETA: 40s - loss: 0.3671 - acc: 0.8383\n1189120/1200000 [============================&gt;.] - ETA: 39s - loss: 0.3670 - acc: 0.8383\n1189248/1200000 [============================&gt;.] - ETA: 39s - loss: 0.3670 - acc: 0.8383\n1189376/1200000 [============================&gt;.] - ETA: 38s - loss: 0.3671 - acc: 0.8383\n1189504/1200000 [============================&gt;.] - ETA: 38s - loss: 0.3671 - acc: 0.8383\n1189632/1200000 [============================&gt;.] - ETA: 38s - loss: 0.3671 - acc: 0.8383\n1189760/1200000 [============================&gt;.] - ETA: 37s - loss: 0.3671 - acc: 0.8383\n1189888/1200000 [============================&gt;.] - ETA: 37s - loss: 0.3670 - acc: 0.8383\n1190016/1200000 [============================&gt;.] - ETA: 36s - loss: 0.3670 - acc: 0.8383\n1190144/1200000 [============================&gt;.] - ETA: 36s - loss: 0.3670 - acc: 0.8383\n1190272/1200000 [============================&gt;.] - ETA: 35s - loss: 0.3670 - acc: 0.8383\n1190400/1200000 [============================&gt;.] - ETA: 35s - loss: 0.3671 - acc: 0.8383\n1190528/1200000 [============================&gt;.] - ETA: 34s - loss: 0.3671 - acc: 0.8383\n1190656/1200000 [============================&gt;.] - ETA: 34s - loss: 0.3671 - acc: 0.8383\n1190784/1200000 [============================&gt;.] - ETA: 33s - loss: 0.3671 - acc: 0.8383\n1190912/1200000 [============================&gt;.] - ETA: 33s - loss: 0.3671 - acc: 0.8383\n1191040/1200000 [============================&gt;.] - ETA: 32s - loss: 0.3671 - acc: 0.8383\n1191168/1200000 [============================&gt;.] - ETA: 32s - loss: 0.3670 - acc: 0.8383\n1191296/1200000 [============================&gt;.] - ETA: 31s - loss: 0.3670 - acc: 0.8383\n1191424/1200000 [============================&gt;.] - ETA: 31s - loss: 0.3670 - acc: 0.8383\n1191552/1200000 [============================&gt;.] - ETA: 30s - loss: 0.3670 - acc: 0.8383\n1191680/1200000 [============================&gt;.] - ETA: 30s - loss: 0.3670 - acc: 0.8383\n1191808/1200000 [============================&gt;.] - ETA: 30s - loss: 0.3670 - acc: 0.8383\n1191936/1200000 [============================&gt;.] - ETA: 29s - loss: 0.3670 - acc: 0.8383\n1192064/1200000 [============================&gt;.] - ETA: 29s - loss: 0.3670 - acc: 0.8383\n1192192/1200000 [============================&gt;.] - ETA: 28s - loss: 0.3670 - acc: 0.8383\n1192320/1200000 [============================&gt;.] - ETA: 28s - loss: 0.3670 - acc: 0.8383\n1192448/1200000 [============================&gt;.] - ETA: 27s - loss: 0.3670 - acc: 0.8383\n1192576/1200000 [============================&gt;.] - ETA: 27s - loss: 0.3670 - acc: 0.8383\n1192704/1200000 [============================&gt;.] - ETA: 26s - loss: 0.3670 - acc: 0.8383\n1192832/1200000 [============================&gt;.] - ETA: 26s - loss: 0.3670 - acc: 0.8383\n1192960/1200000 [============================&gt;.] - ETA: 25s - loss: 0.3670 - acc: 0.8383\n1193088/1200000 [============================&gt;.] - ETA: 25s - loss: 0.3670 - acc: 0.8383\n1193216/1200000 [============================&gt;.] - ETA: 24s - loss: 0.3670 - acc: 0.8383\n1193344/1200000 [============================&gt;.] - ETA: 24s - loss: 0.3670 - acc: 0.8383\n1193472/1200000 [============================&gt;.] - ETA: 23s - loss: 0.3670 - acc: 0.8383\n1193600/1200000 [============================&gt;.] - ETA: 23s - loss: 0.3670 - acc: 0.8384\n1193728/1200000 [============================&gt;.] - ETA: 23s - loss: 0.3670 - acc: 0.8383\n1193856/1200000 [============================&gt;.] - ETA: 22s - loss: 0.3670 - acc: 0.8383\n1193984/1200000 [============================&gt;.] - ETA: 22s - loss: 0.3670 - acc: 0.8383\n1194112/1200000 [============================&gt;.] - ETA: 21s - loss: 0.3670 - acc: 0.8384\n1194240/1200000 [============================&gt;.] - ETA: 21s - loss: 0.3670 - acc: 0.8384\n1194368/1200000 [============================&gt;.] - ETA: 20s - loss: 0.3670 - acc: 0.8384\n1194496/1200000 [============================&gt;.] - ETA: 20s - loss: 0.3670 - acc: 0.8384\n1194624/1200000 [============================&gt;.] - ETA: 19s - loss: 0.3670 - acc: 0.8383\n1194752/1200000 [============================&gt;.] - ETA: 19s - loss: 0.3670 - acc: 0.8383\n1194880/1200000 [============================&gt;.] - ETA: 18s - loss: 0.3670 - acc: 0.8383\n1195008/1200000 [============================&gt;.] - ETA: 18s - loss: 0.3670 - acc: 0.8383\n1195136/1200000 [============================&gt;.] - ETA: 17s - loss: 0.3670 - acc: 0.8383\n1195264/1200000 [============================&gt;.] - ETA: 17s - loss: 0.3670 - acc: 0.8384\n1195392/1200000 [============================&gt;.] - ETA: 16s - loss: 0.3670 - acc: 0.8384\n1195520/1200000 [============================&gt;.] - ETA: 16s - loss: 0.3670 - acc: 0.8384\n1195648/1200000 [============================&gt;.] - ETA: 15s - loss: 0.3670 - acc: 0.8383\n1195776/1200000 [============================&gt;.] - ETA: 15s - loss: 0.3670 - acc: 0.8383\n1195904/1200000 [============================&gt;.] - ETA: 15s - loss: 0.3670 - acc: 0.8383\n1196032/1200000 [============================&gt;.] - ETA: 14s - loss: 0.3670 - acc: 0.8383\n1196160/1200000 [============================&gt;.] - ETA: 14s - loss: 0.3670 - acc: 0.8383\n1196288/1200000 [============================&gt;.] - ETA: 13s - loss: 0.3670 - acc: 0.8383\n1196416/1200000 [============================&gt;.] - ETA: 13s - loss: 0.3670 - acc: 0.8383\n1196544/1200000 [============================&gt;.] - ETA: 12s - loss: 0.3670 - acc: 0.8383\n1196672/1200000 [============================&gt;.] - ETA: 12s - loss: 0.3670 - acc: 0.8383\n1196800/1200000 [============================&gt;.] - ETA: 11s - loss: 0.3670 - acc: 0.8383\n1196928/1200000 [============================&gt;.] - ETA: 11s - loss: 0.3670 - acc: 0.8383\n1197056/1200000 [============================&gt;.] - ETA: 10s - loss: 0.3670 - acc: 0.8383\n1197184/1200000 [============================&gt;.] - ETA: 10s - loss: 0.3670 - acc: 0.8383\n1197312/1200000 [============================&gt;.] - ETA: 9s - loss: 0.3670 - acc: 0.8383 \n1197440/1200000 [============================&gt;.] - ETA: 9s - loss: 0.3670 - acc: 0.8383\n1197568/1200000 [============================&gt;.] - ETA: 8s - loss: 0.3670 - acc: 0.8383\n1197696/1200000 [============================&gt;.] - ETA: 8s - loss: 0.3670 - acc: 0.8383\n1197824/1200000 [============================&gt;.] - ETA: 7s - loss: 0.3670 - acc: 0.8383\n1197952/1200000 [============================&gt;.] - ETA: 7s - loss: 0.3670 - acc: 0.8383\n1198080/1200000 [============================&gt;.] - ETA: 7s - loss: 0.3670 - acc: 0.8383\n1198208/1200000 [============================&gt;.] - ETA: 6s - loss: 0.3671 - acc: 0.8383\n1198336/1200000 [============================&gt;.] - ETA: 6s - loss: 0.3671 - acc: 0.8383\n1198464/1200000 [============================&gt;.] - ETA: 5s - loss: 0.3671 - acc: 0.8383\n1198592/1200000 [============================&gt;.] - ETA: 5s - loss: 0.3671 - acc: 0.8383\n1198720/1200000 [============================&gt;.] - ETA: 4s - loss: 0.3671 - acc: 0.8383\n1198848/1200000 [============================&gt;.] - ETA: 4s - loss: 0.3671 - acc: 0.8383\n1198976/1200000 [============================&gt;.] - ETA: 3s - loss: 0.3671 - acc: 0.8383\n1199104/1200000 [============================&gt;.] - ETA: 3s - loss: 0.3671 - acc: 0.8383\n1199232/1200000 [============================&gt;.] - ETA: 2s - loss: 0.3671 - acc: 0.8383\n1199360/1200000 [============================&gt;.] - ETA: 2s - loss: 0.3671 - acc: 0.8383\n1199488/1200000 [============================&gt;.] - ETA: 1s - loss: 0.3671 - acc: 0.8383\n1199616/1200000 [============================&gt;.] - ETA: 1s - loss: 0.3671 - acc: 0.8383\n1199744/1200000 [============================&gt;.] - ETA: 0s - loss: 0.3671 - acc: 0.8383\n1199872/1200000 [============================&gt;.] - ETA: 0s - loss: 0.3671 - acc: 0.8383\n1200000/1200000 [==============================] - 4550s 4ms/step - loss: 0.3671 - acc: 0.8383 - val_loss: 0.3851 - val_acc: 0.8270\nOut[33]: &lt;keras.callbacks.History at 0x7fbad1140e48&gt;</div>"]}}],"execution_count":48},{"cell_type":"markdown","source":["Now, let us save the model architecture and weights."],"metadata":{}},{"cell_type":"code","source":["# Save the weights\nmodel.save_weights('/dbfs/FileStore/tables/model_weights.h5')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["# Save the model architecture\n\nwith open('/dbfs/FileStore/tables/model_architecture.json', 'w') as f:\n    f.write(model.to_json())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["END"],"metadata":{}}],"metadata":{"name":"Transfer Learning Pipeline","notebookId":2867773054688686},"nbformat":4,"nbformat_minor":0}
