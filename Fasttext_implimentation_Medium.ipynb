{"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n\"\"\"\n\nCode implementing a fastText model training.\n\n@author: Charles Malafosse - BetSentiment.com\nhttps://betsentiment.com\n\n\"\"\"\n\nimport fastText\nimport sys\nimport os\nimport nltk\nnltk.download('punkt')\nimport csv\nimport datetime\nfrom bs4 import BeautifulSoup\nimport re\nimport itertools\nimport emoji\n\n\n#####################################################################################\n#\n# DATA CLEANING\n#\n#####################################################################################\n\ndef load_dict_smileys():\n    \n    return {\n        \":‑)\":\"smiley\",\n        \":-]\":\"smiley\",\n        \":-3\":\"smiley\",\n        \":->\":\"smiley\",\n        \"8-)\":\"smiley\",\n        \":-}\":\"smiley\",\n        \":)\":\"smiley\",\n        \":]\":\"smiley\",\n        \":3\":\"smiley\",\n        \":>\":\"smiley\",\n        \"8)\":\"smiley\",\n        \":}\":\"smiley\",\n        \":o)\":\"smiley\",\n        \":c)\":\"smiley\",\n        \":^)\":\"smiley\",\n        \"=]\":\"smiley\",\n        \"=)\":\"smiley\",\n        \":-))\":\"smiley\",\n        \":‑D\":\"smiley\",\n        \"8‑D\":\"smiley\",\n        \"x‑D\":\"smiley\",\n        \"X‑D\":\"smiley\",\n        \":D\":\"smiley\",\n        \"8D\":\"smiley\",\n        \"xD\":\"smiley\",\n        \"XD\":\"smiley\",\n        \":‑(\":\"sad\",\n        \":‑c\":\"sad\",\n        \":‑<\":\"sad\",\n        \":‑[\":\"sad\",\n        \":(\":\"sad\",\n        \":c\":\"sad\",\n        \":<\":\"sad\",\n        \":[\":\"sad\",\n        \":-||\":\"sad\",\n        \">:[\":\"sad\",\n        \":{\":\"sad\",\n        \":@\":\"sad\",\n        \">:(\":\"sad\",\n        \":'‑(\":\"sad\",\n        \":'(\":\"sad\",\n        \":‑P\":\"playful\",\n        \"X‑P\":\"playful\",\n        \"x‑p\":\"playful\",\n        \":‑p\":\"playful\",\n        \":‑Þ\":\"playful\",\n        \":‑þ\":\"playful\",\n        \":‑b\":\"playful\",\n        \":P\":\"playful\",\n        \"XP\":\"playful\",\n        \"xp\":\"playful\",\n        \":p\":\"playful\",\n        \":Þ\":\"playful\",\n        \":þ\":\"playful\",\n        \":b\":\"playful\",\n        \"<3\":\"love\"\n        }\n\n\ndef load_dict_contractions():\n    \n    return {\n        \"ain't\":\"is not\",\n        \"amn't\":\"am not\",\n        \"aren't\":\"are not\",\n        \"can't\":\"cannot\",\n        \"'cause\":\"because\",\n        \"couldn't\":\"could not\",\n        \"couldn't've\":\"could not have\",\n        \"could've\":\"could have\",\n        \"daren't\":\"dare not\",\n        \"daresn't\":\"dare not\",\n        \"dasn't\":\"dare not\",\n        \"didn't\":\"did not\",\n        \"doesn't\":\"does not\",\n        \"don't\":\"do not\",\n        \"e'er\":\"ever\",\n        \"em\":\"them\",\n        \"everyone's\":\"everyone is\",\n        \"finna\":\"fixing to\",\n        \"gimme\":\"give me\",\n        \"gonna\":\"going to\",\n        \"gon't\":\"go not\",\n        \"gotta\":\"got to\",\n        \"hadn't\":\"had not\",\n        \"hasn't\":\"has not\",\n        \"haven't\":\"have not\",\n        \"he'd\":\"he would\",\n        \"he'll\":\"he will\",\n        \"he's\":\"he is\",\n        \"he've\":\"he have\",\n        \"how'd\":\"how would\",\n        \"how'll\":\"how will\",\n        \"how're\":\"how are\",\n        \"how's\":\"how is\",\n        \"I'd\":\"I would\",\n        \"I'll\":\"I will\",\n        \"I'm\":\"I am\",\n        \"I'm'a\":\"I am about to\",\n        \"I'm'o\":\"I am going to\",\n        \"isn't\":\"is not\",\n        \"it'd\":\"it would\",\n        \"it'll\":\"it will\",\n        \"it's\":\"it is\",\n        \"I've\":\"I have\",\n        \"kinda\":\"kind of\",\n        \"let's\":\"let us\",\n        \"mayn't\":\"may not\",\n        \"may've\":\"may have\",\n        \"mightn't\":\"might not\",\n        \"might've\":\"might have\",\n        \"mustn't\":\"must not\",\n        \"mustn't've\":\"must not have\",\n        \"must've\":\"must have\",\n        \"needn't\":\"need not\",\n        \"ne'er\":\"never\",\n        \"o'\":\"of\",\n        \"o'er\":\"over\",\n        \"ol'\":\"old\",\n        \"oughtn't\":\"ought not\",\n        \"shalln't\":\"shall not\",\n        \"shan't\":\"shall not\",\n        \"she'd\":\"she would\",\n        \"she'll\":\"she will\",\n        \"she's\":\"she is\",\n        \"shouldn't\":\"should not\",\n        \"shouldn't've\":\"should not have\",\n        \"should've\":\"should have\",\n        \"somebody's\":\"somebody is\",\n        \"someone's\":\"someone is\",\n        \"something's\":\"something is\",\n        \"that'd\":\"that would\",\n        \"that'll\":\"that will\",\n        \"that're\":\"that are\",\n        \"that's\":\"that is\",\n        \"there'd\":\"there would\",\n        \"there'll\":\"there will\",\n        \"there're\":\"there are\",\n        \"there's\":\"there is\",\n        \"these're\":\"these are\",\n        \"they'd\":\"they would\",\n        \"they'll\":\"they will\",\n        \"they're\":\"they are\",\n        \"they've\":\"they have\",\n        \"this's\":\"this is\",\n        \"those're\":\"those are\",\n        \"'tis\":\"it is\",\n        \"'twas\":\"it was\",\n        \"wanna\":\"want to\",\n        \"wasn't\":\"was not\",\n        \"we'd\":\"we would\",\n        \"we'd've\":\"we would have\",\n        \"we'll\":\"we will\",\n        \"we're\":\"we are\",\n        \"weren't\":\"were not\",\n        \"we've\":\"we have\",\n        \"what'd\":\"what did\",\n        \"what'll\":\"what will\",\n        \"what're\":\"what are\",\n        \"what's\":\"what is\",\n        \"what've\":\"what have\",\n        \"when's\":\"when is\",\n        \"where'd\":\"where did\",\n        \"where're\":\"where are\",\n        \"where's\":\"where is\",\n        \"where've\":\"where have\",\n        \"which's\":\"which is\",\n        \"who'd\":\"who would\",\n        \"who'd've\":\"who would have\",\n        \"who'll\":\"who will\",\n        \"who're\":\"who are\",\n        \"who's\":\"who is\",\n        \"who've\":\"who have\",\n        \"why'd\":\"why did\",\n        \"why're\":\"why are\",\n        \"why's\":\"why is\",\n        \"won't\":\"will not\",\n        \"wouldn't\":\"would not\",\n        \"would've\":\"would have\",\n        \"y'all\":\"you all\",\n        \"you'd\":\"you would\",\n        \"you'll\":\"you will\",\n        \"you're\":\"you are\",\n        \"you've\":\"you have\",\n        \"Whatcha\":\"What are you\",\n        \"luv\":\"love\",\n        \"sux\":\"sucks\"\n        }\n\n\ndef strip_accents(text):\n    if 'ø' in text or  'Ø' in text:\n        #Do nothing when finding ø \n        return text   \n    text = text.encode('ascii', 'ignore')\n    text = text.decode(\"utf-8\")\n    return str(text)\n\n\ndef tweet_cleaning_for_sentiment_analysis(tweet):    \n    \n    #Escaping HTML characters\n    tweet = BeautifulSoup(tweet).get_text()\n    #Special case not handled previously.\n    tweet = tweet.replace('\\x92',\"'\")\n    #Removal of hastags/account\n    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n    #Removal of address\n    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n    #Removal of Punctuation\n    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n    #Lower case\n    tweet = tweet.lower()\n    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n    CONTRACTIONS = load_dict_contractions()\n    tweet = tweet.replace(\"’\",\"'\")\n    words = tweet.split()\n    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n    tweet = \" \".join(reformed)\n    # Standardizing words\n    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n    #Deal with smileys\n    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n    SMILEY = load_dict_smileys()  \n    words = tweet.split()\n    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n    tweet = \" \".join(reformed)\n    #Deal with emojis\n    tweet = emoji.demojize(tweet)\n    #Strip accents\n    tweet= strip_accents(tweet)\n    tweet = tweet.replace(\":\",\" \")\n    tweet = ' '.join(tweet.split())\n    \n    # DO NOT REMOVE STOP WORDS FOR SENTIMENT ANALYSIS - OR AT LEAST NOT NEGATIVE ONES\n\n    return tweet\n\n\n\n#####################################################################################\n#\n# DATA PROCESSING\n#\n#####################################################################################\n\ndef transform_instance(row):\n    cur_row = []\n    #Prefix the index-ed label with __label__\n    label = \"__label__\" + row[4]  \n    cur_row.append(label)\n    cur_row.extend(nltk.word_tokenize(tweet_cleaning_for_sentiment_analysis(row[2].lower())))\n    return cur_row\n\n\ndef preprocess(input_file, output_file, keep=1):\n    i=0\n    with open(output_file, 'w') as csvoutfile:\n        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n        with open(input_file, 'r', newline='') as csvinfile: #,encoding='latin1'\n            csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n            for row in csv_reader:\n                if row[4]!=\"MIXED\" and row[4].upper() in ['POSITIVE','NEGATIVE','NEUTRAL'] and row[2]!='':\n                    row_output = transform_instance(row)\n                    csv_writer.writerow(row_output )\n                i=i+1\n                if i%10000 ==0:\n                    print(i)\n            \n# Preparing the training dataset        \npreprocess('betsentiment-EN-tweets-sentiment-teams.csv', 'tweets.train')\n\n# Preparing the validation dataset        \npreprocess('betsentiment-EN-tweets-sentiment-players.csv', 'tweets.validation')\n\n\n#####################################################################################\n#\n# UPSAMPLING\n#\n#####################################################################################\n\ndef upsampling(input_file, output_file, ratio_upsampling=1):\n    # Create a file with equal number of tweets for each label\n    #    input_file: path to file\n    #    output_file: path to the output file\n    #    ratio_upsampling: ratio of each minority classes vs majority one. 1 mean there will be as much of each class than there is for the majority class \n    \n    i=0\n    counts = {}\n    dict_data_by_label = {}\n\n    # GET LABEL LIST AND GET DATA PER LABEL\n    with open(input_file, 'r', newline='') as csvinfile: \n        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n        for row in csv_reader:\n            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1\n            if not row[0].split()[0] in dict_data_by_label:\n                dict_data_by_label[row[0].split()[0]]=[row[0]]\n            else:\n                dict_data_by_label[row[0].split()[0]].append(row[0])\n            i=i+1\n            if i%10000 ==0:\n                print(\"read\" + str(i))\n\n    # FIND MAJORITY CLASS\n    majority_class=\"\"\n    count_majority_class=0\n    for item in dict_data_by_label:\n        if len(dict_data_by_label[item])>count_majority_class:\n            majority_class= item\n            count_majority_class=len(dict_data_by_label[item])  \n    \n    # UPSAMPLE MINORITY CLASS\n    data_upsampled=[]\n    for item in dict_data_by_label:\n        data_upsampled.extend(dict_data_by_label[item])\n        if item != majority_class:\n            items_added=0\n            items_to_add = count_majority_class - len(dict_data_by_label[item])\n            while items_added<items_to_add:\n                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])\n                items_added = items_added + max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))\n\n    # WRITE ALL\n    i=0\n\n    with open(output_file, 'w') as txtoutfile:\n        for row in data_upsampled:\n            txtoutfile.write(row+ '\\n' )\n            i=i+1\n            if i%10000 ==0:\n                print(\"writer\" + str(i))\n\n\nupsampling( 'tweets.train','uptweets.train')\n# No need to upsample for the validation set.\n\n\n#####################################################################################\n#\n# TRAINING\n#\n#####################################################################################\n\n# Full path to training data.\ntraining_data_path ='C:\\\\Users\\\\charl\\\\Dropbox\\\\Bet Sentiment\\\\SENTIMENT_ANALYSIS\\\\uptweets.train' \nvalidation_data_path ='C:\\\\Users\\\\charl\\\\Dropbox\\\\Bet Sentiment\\\\SENTIMENT_ANALYSIS\\\\tweets.validation'\nmodel_path ='\\\\model\\\\'\nmodel_name=\"model-en\"\n\ndef train():\n    print('Training start')\n    try:\n        hyper_params = {\"lr\": 0.01,\n                        \"epoch\": 20,\n                        \"wordNgrams\": 2,\n                        \"dim\": 20}     \n                               \n        print(str(datetime.datetime.now()) + ' START=>' + str(hyper_params) )\n\n        # Train the model.\n        model = fastText.train_supervised(input=training_data_path, **hyper_params)\n        print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))\n\n        # CHECK PERFORMANCE\n        print(str(datetime.datetime.now()) + 'Training complete.' + str(hyper_params) )\n        \n        result = model.test(training_data_path)\n        validation = model.test(validation_data_path)\n        \n        # DISPLAY ACCURACY OF TRAINED MODEL\n        text_line = str(hyper_params) + \",accuracy:\" + str(result[1])  + \",validation:\" + str(validation[1]) + '\\n' \n        print(text_line)\n        \n        #quantize a model to reduce the memory usage\n        model.quantize(input=training_data_path, qnorm=True, retrain=True, cutoff=100000)\n        print(\"Model is quantized!!\")\n        model.save_model(os.path.join(model_path,model_name + \".ftz\"))                \n    \n        ##########################################################################\n        #\n        #  TESTING PART\n        #\n        ##########################################################################            \n        model.predict(['why not'],k=3)\n        model.predict(['this player is so bad'],k=1)\n        \n    except Exception as e:\n        print('Exception during training: ' + str(e) )\n\n\n# Train your model.\ntrain()"],"metadata":{},"outputs":[],"execution_count":1}],"metadata":{"name":"Fasttext_implimentation_Medium","notebookId":913008479835647},"nbformat":4,"nbformat_minor":0}
